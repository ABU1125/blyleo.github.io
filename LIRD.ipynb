{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIRD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gwoaes5UyJJS",
        "gfGFFQeZyQfG",
        "ZGvAXnJayWjt",
        "3-rEdckqygGY",
        "lVAPvO0Nyyy3",
        "KNP3jdofyh7M"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ABU1125/blyleo.github.io/blob/main/LIRD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VCoNiElLnK4"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKmjjI_7qfaP"
      },
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import keras\n",
        "# import tensorflow.keras.backend as K\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Dropout"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwoaes5UyJJS"
      },
      "source": [
        "# MovieLens 100k Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2H9eM4YuOy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db32c732-e751-4d68-9b2a-cb8e47557b64"
      },
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip -q ml-100k.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-20 12:46:37--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip’\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  24.8MB/s    in 0.2s    \n",
            "\n",
            "2025-02-20 12:46:37 (24.8 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yST4dc3wtm2T"
      },
      "source": [
        "class DataGenerator():\n",
        "  def __init__(self, datapath, itempath):\n",
        "    '''\n",
        "    Load data from the DB MovieLens\n",
        "    List the users and the items\n",
        "    List all the users historic\n",
        "    '''\n",
        "    self.data  = self.load_datas(datapath, itempath)\n",
        "    self.users = self.data['userId'].unique()   #list of all users\n",
        "    self.items = self.data['itemId'].unique()   #list of all items\n",
        "    self.histo = self.gen_histo()\n",
        "    self.train = []\n",
        "    self.test  = []\n",
        "\n",
        "  def load_datas(self, datapath, itempath):\n",
        "    '''\n",
        "    Load the data and merge the name of each movie.\n",
        "    A row corresponds to a rate given by a user to a movie.\n",
        "\n",
        "     Parameters\n",
        "    ----------\n",
        "    datapath :  string\n",
        "                path to the data 100k MovieLens\n",
        "                contains usersId;itemId;rating\n",
        "    itempath :  string\n",
        "                path to the data 100k MovieLens\n",
        "                contains itemId;itemName\n",
        "     Returns\n",
        "    -------\n",
        "    result :    DataFrame\n",
        "                Contains all the ratings\n",
        "    '''\n",
        "    data = pd.read_csv(datapath, sep='\\t',\n",
        "                       names=['userId', 'itemId', 'rating', 'timestamp'])\n",
        "    movie_titles = pd.read_csv(itempath, sep='|', names=['itemId', 'itemName'],\n",
        "                           usecols=range(2), encoding='latin-1')\n",
        "    return data.merge(movie_titles,on='itemId', how='left')\n",
        "\n",
        "\n",
        "  def gen_histo(self):\n",
        "    '''\n",
        "    Group all rates given by users and store them from older to most recent.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    result :    List(DataFrame)\n",
        "                List of the historic for each user\n",
        "    '''\n",
        "    historic_users = []\n",
        "    for i, u in enumerate(self.users):\n",
        "      temp = self.data[self.data['userId'] == u]\n",
        "      temp = temp.sort_values('timestamp').reset_index()\n",
        "      temp.drop('index', axis=1, inplace=True)\n",
        "      historic_users.append(temp)\n",
        "    return historic_users\n",
        "\n",
        "  def sample_histo(self, user_histo, action_ratio=0.8, max_samp_by_user=5,  max_state=100, max_action=50, nb_states=[], nb_actions=[]):\n",
        "    '''\n",
        "    For a given historic, make one or multiple sampling.\n",
        "    If no optional argument given for nb_states and nb_actions, then the sampling\n",
        "    is random and each sample can have differents size for action and state.\n",
        "    To normalize sampling we need to give list of the numbers of states and actions\n",
        "    to be sampled.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    user_histo :  DataFrame\n",
        "                      historic of user\n",
        "    delimiter :       string, optional\n",
        "                      delimiter for the csv\n",
        "    action_ratio :    float, optional\n",
        "                      ratio form which movies in history will be selected\n",
        "    max_samp_by_user: int, optional\n",
        "                      Nulber max of sample to make by user\n",
        "    max_state :       int, optional\n",
        "                      Number max of movies to take for the 'state' column\n",
        "    max_action :      int, optional\n",
        "                      Number max of movies to take for the 'action' action\n",
        "    nb_states :       array(int), optional\n",
        "                      Numbers of movies to be taken for each sample made on user's historic\n",
        "    nb_actions :      array(int), optional\n",
        "                      Numbers of rating to be taken for each sample made on user's historic\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    states :         List(String)\n",
        "                     All the states sampled, format of a sample: itemId&rating\n",
        "    actions :        List(String)\n",
        "                     All the actions sampled, format of a sample: itemId&rating\n",
        "\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    States must be before(timestamp<) the actions.\n",
        "    If given, size of nb_states is the numbller of sample by user\n",
        "    sizes of nb_states and nb_actions must be equals\n",
        "    '''\n",
        "\n",
        "    n = len(user_histo)\n",
        "    sep = int(action_ratio * n)\n",
        "    nb_sample = random.randint(1, max_samp_by_user)\n",
        "    if not nb_states:\n",
        "      nb_states = [min(random.randint(1, sep), max_state) for i in range(nb_sample)]\n",
        "    if not nb_actions:\n",
        "      nb_actions = [min(random.randint(1, n - sep), max_action) for i in range(nb_sample)]\n",
        "    assert len(nb_states) == len(nb_actions), 'Given array must have the same size'\n",
        "\n",
        "    states  = []\n",
        "    actions = []\n",
        "    # SELECT SAMPLES IN HISTO\n",
        "    for i in range(len(nb_states)):\n",
        "      sample_states = user_histo.iloc[0:sep].sample(nb_states[i])\n",
        "      sample_actions = user_histo.iloc[-(n - sep):].sample(nb_actions[i])\n",
        "\n",
        "      sample_state =  []\n",
        "      sample_action = []\n",
        "      for j in range(nb_states[i]):\n",
        "        row   = sample_states.iloc[j]\n",
        "        # FORMAT STATE\n",
        "        state = str(row.loc['itemId']) + '&' + str(row.loc['rating'])\n",
        "        sample_state.append(state)\n",
        "\n",
        "      for j in range(nb_actions[i]):\n",
        "        row    = sample_actions.iloc[j]\n",
        "        # FORMAT ACTION\n",
        "        action = str(row.loc['itemId']) + '&' + str(row.loc['rating'])\n",
        "        sample_action.append(action)\n",
        "\n",
        "      states.append(sample_state)\n",
        "      actions.append(sample_action)\n",
        "    return states, actions\n",
        "\n",
        "  def gen_train_test(self, test_ratio, seed=None):\n",
        "    '''\n",
        "    Shuffle the historic of users and separate it in a train and a test set.\n",
        "    Store the ids for each set.\n",
        "    An user can't be in both set.\n",
        "\n",
        "     Parameters\n",
        "    ----------\n",
        "    test_ratio :  float\n",
        "                  Ratio to control the sizes of the sets\n",
        "    seed       :  float\n",
        "                  Seed on the shuffle\n",
        "    '''\n",
        "    n = len(self.histo)\n",
        "\n",
        "    if seed is not None:\n",
        "      random.Random(seed).shuffle(self.histo)\n",
        "    else:\n",
        "      random.shuffle(self.histo)\n",
        "\n",
        "    self.train = self.histo[:int((test_ratio * n))]\n",
        "    self.test  = self.histo[int((test_ratio * n)):]\n",
        "    self.user_train = [h.iloc[0,0] for h in self.train]\n",
        "    self.user_test  = [h.iloc[0,0] for h in self.test]\n",
        "\n",
        "\n",
        "  def write_csv(self, filename, histo_to_write, delimiter=';', action_ratio=0.8, max_samp_by_user=5, max_state=100, max_action=50, nb_states=[], nb_actions=[]):\n",
        "    '''\n",
        "    From  a given historic, create a csv file with the format:\n",
        "    columns : state;action_reward;n_state\n",
        "    rows    : itemid&rating1 | itemid&rating2 | ... ; itemid&rating3 | ... | itemid&rating4; itemid&rating1 | itemid&rating2 | itemid&rating3 | ... | item&rating4\n",
        "    at filename location.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename :        string\n",
        "                      path to the file to be produced\n",
        "    histo_to_write :  List(DataFrame)\n",
        "                      List of the historic for each user\n",
        "    delimiter :       string, optional\n",
        "                      delimiter for the csv\n",
        "    action_ratio :    float, optional\n",
        "                      ratio form which movies in history will be selected\n",
        "    max_samp_by_user: int, optional\n",
        "                      Nulber max of sample to make by user\n",
        "    max_state :       int, optional\n",
        "                      Number max of movies to take for the 'state' column\n",
        "    max_action :      int, optional\n",
        "                      Number max of movies to take for the 'action' action\n",
        "    nb_states :       array(int), optional\n",
        "                      Numbers of movies to be taken for each sample made on user's historic\n",
        "    nb_actions :      array(int), optional\n",
        "                      Numbers of rating to be taken for each sample made on user's historic\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    if given, size of nb_states is the numbller of sample by user\n",
        "    sizes of nb_states and nb_actions must be equals\n",
        "\n",
        "    '''\n",
        "    with open(filename, mode='w') as file:\n",
        "      f_writer = csv.writer(file, delimiter=delimiter)\n",
        "      f_writer.writerow(['state', 'action_reward', 'n_state'])\n",
        "      for user_histo in histo_to_write:\n",
        "        states, actions = self.sample_histo(user_histo, action_ratio, max_samp_by_user, max_state, max_action, nb_states, nb_actions)\n",
        "        for i in range(len(states)):\n",
        "          # FORMAT STATE\n",
        "          state_str   = '|'.join(states[i])\n",
        "          # FORMAT ACTION\n",
        "          action_str  = '|'.join(actions[i])\n",
        "          # FORMAT N_STATE\n",
        "          n_state_str = state_str + '|' + action_str\n",
        "          f_writer.writerow([state_str, action_str, n_state_str])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfGFFQeZyQfG"
      },
      "source": [
        "# Movies Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "578-inzm24sj"
      },
      "source": [
        "class EmbeddingsGenerator:\n",
        "  def  __init__(self, train_users, data):\n",
        "    self.train_users = train_users\n",
        "\n",
        "    #preprocess\n",
        "    self.data = data.sort_values(by=['timestamp'])\n",
        "    #使ID从0开始\n",
        "    self.data['userId'] = self.data['userId'] - 1\n",
        "    self.data['itemId'] = self.data['itemId'] - 1\n",
        "    # 统计用户和电影数量\n",
        "    self.user_count = self.data['userId'].max() + 1\n",
        "    self.movie_count = self.data['itemId'].max() + 1\n",
        "    # 记录每个用户看过的电影\n",
        "    self.user_movies = {} #list of rated movies by each user\n",
        "    for userId in range(self.user_count):\n",
        "      self.user_movies[userId] = self.data[self.data.userId == userId]['itemId'].tolist()\n",
        "    # 构建神经网络\n",
        "    self.m = self.model()\n",
        "\n",
        "\n",
        "  def model(self, hidden_layer_size=100):\n",
        "    '''\n",
        "    hidden_layer_size：表示隐藏层的神经元数量。\n",
        "    '''\n",
        "\n",
        "    m = Sequential() # 创建顺序模型\n",
        "    m.add(Dense(hidden_layer_size, input_shape=(1, self.movie_count))) # 全连接层 Dense(units=输出维度, activation=激活函数, input_shape=输入维度)\n",
        "    m.add(Dropout(0.2)) # 随机丢弃层 丢弃 20% 的神经元\n",
        "    m.add(Dense(self.movie_count, activation='softmax')) # 输出各电影的概率\n",
        "    m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # 准确率评估\n",
        "    return m\n",
        "\n",
        "  def generate_input(self, user_id):\n",
        "    '''\n",
        "    Returns a context and a target for the user_id\n",
        "    context: user's history with one random movie removed\n",
        "    target: id of random removed movie\n",
        "    模拟一个预测任务\n",
        "    '''\n",
        "    user_movies_count = len(self.user_movies[user_id])\n",
        "    #随机选择一个电影作为目标\n",
        "    random_index = np.random.randint(0, user_movies_count-1) # -1 avoids taking the last movie\n",
        "    #setting target\n",
        "    target = np.zeros((1, self.movie_count))\n",
        "    target[0][self.user_movies[user_id][random_index]] = 1\n",
        "    #setting context\n",
        "    context = np.zeros((1, self.movie_count))\n",
        "    context[0][self.user_movies[user_id][:random_index] + self.user_movies[user_id][random_index+1:]] = 1\n",
        "    return context, target\n",
        "\n",
        "  def train(self, nb_epochs = 300, batch_size = 10000):\n",
        "    '''\n",
        "    Trains the model from train_users's history\n",
        "    nb_epochs是迭代次数，batch_size是每批的样本数\n",
        "    在每个epoch中，生成一批训练数据，X_train是上下文，y_train是目标，然后调用模型的fit方法进行训练\n",
        "    '''\n",
        "    for i in range(nb_epochs):\n",
        "      print('%d/%d' % (i+1, nb_epochs))\n",
        "      batch = [self.generate_input(user_id=np.random.choice(self.train_users) - 1) for _ in range(batch_size)]\n",
        "      X_train = np.array([b[0] for b in batch])\n",
        "      y_train = np.array([b[1] for b in batch])\n",
        "      self.m.fit(X_train, y_train, epochs=1, validation_split=0.5)\n",
        "\n",
        "  def test(self, test_users, batch_size = 100000):\n",
        "    '''\n",
        "    Returns [loss, accuracy] on the test set\n",
        "    '''\n",
        "    batch_test = [self.generate_input(user_id=np.random.choice(test_users) - 1) for _ in range(batch_size)]\n",
        "    X_test = np.array([b[0] for b in batch_test])\n",
        "    y_test = np.array([b[1] for b in batch_test])\n",
        "    return self.m.evaluate(X_test, y_test)\n",
        "\n",
        "  # def save_embeddings(self, file_name):\n",
        "  #   '''\n",
        "  #   Generates a csv file containg the vector embedding for each movie.\n",
        "  #   '''\n",
        "\n",
        "  #   inp = [layer.input for layer in self.m.layers]\n",
        "  #   #inp = self.m.input                                           # input placeholder 获取模型的输入张量\n",
        "  #   outputs = [layer.output for layer in self.m.layers]          # all layer outputs 使用列表推导式获取模型中每一层的输出张量\n",
        "  #   functor = K.function([inp, K.learning_phase()], outputs )   # evaluation function 返回模型各层的输出\n",
        "\n",
        "  #   #append embeddings to vectors\n",
        "  #   vectors = []\n",
        "  #   for movie_id in range(self.movie_count):\n",
        "  #     movie = np.zeros((1, 1, self.movie_count))\n",
        "  #     movie[0][0][movie_id] = 1 #电影的独热编码\n",
        "  #     layer_outs = functor([movie])\n",
        "  #     vector = [str(v) for v in layer_outs[0][0][0]] #提取模型第一层输出中对应电影的嵌入向量，并将向量中的每个元素转换为字符串，存储在列表 vector 中\n",
        "  #     # layer_outs[0]：第 0 层（Dense 层）的输出\n",
        "  #     vector = '|'.join(vector)\n",
        "  #     vectors.append([movie_id, vector])\n",
        "\n",
        "  #   #saves as a csv file\n",
        "  #   embeddings = pd.DataFrame(vectors, columns=['item_id', 'vectors']).astype({'item_id': 'int32'})\n",
        "  #   embeddings.to_csv(file_name, sep=';', index=False)\n",
        "  #   files.download(file_name)\n",
        "\n",
        "\n",
        "  def save_embeddings(self, file_name):\n",
        "      '''\n",
        "      Generates a csv file containg the vector embedding for each movie.\n",
        "      '''\n",
        "      # 创建一个子模型，输出为原模型第一层的输出\n",
        "      intermediate_layer_model = tf.keras.Model(inputs=self.m.layers[0].input,outputs=self.m.layers[0].output)\n",
        "\n",
        "      # 存储嵌入向量\n",
        "      vectors = []\n",
        "      for movie_id in range(self.movie_count):\n",
        "          # 创建电影的独热编码输入\n",
        "          movie = np.zeros((1, 1, self.movie_count))\n",
        "          movie[0][0][movie_id] = 1\n",
        "\n",
        "          # 获取中间层的输出\n",
        "          layer_outs = intermediate_layer_model(movie)\n",
        "          vector = [str(v.numpy()) for v in layer_outs[0][0]]\n",
        "          vector = '|'.join(vector)\n",
        "          vectors.append([movie_id, vector])\n",
        "\n",
        "      # 保存为 CSV 文件\n",
        "      embeddings = pd.DataFrame(vectors, columns=['item_id', 'vectors']).astype({'item_id': 'int32'})\n",
        "      embeddings.to_csv(file_name, sep=';', index=False)\n",
        "\n",
        "      try:\n",
        "          # 仅在 Google Colab 中可用\n",
        "          files.download(file_name)\n",
        "      except NameError:\n",
        "          print(\"`files.download` 仅在 Google Colab 中可用。请手动查看保存的文件。\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpKB5xexLvSI"
      },
      "source": [
        "class Embeddings:\n",
        "  def __init__(self, item_embeddings):\n",
        "    self.item_embeddings = item_embeddings\n",
        "\n",
        "  def size(self):\n",
        "    return self.item_embeddings.shape[1]\n",
        "\n",
        "  def get_embedding_vector(self):\n",
        "    return self.item_embeddings\n",
        "\n",
        "  def get_embedding(self, item_index):\n",
        "    return self.item_embeddings[item_index]\n",
        "\n",
        "  def embed(self, item_list):\n",
        "    return np.array([self.get_embedding(item) for item in item_list])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AolIkmf0LWZP"
      },
      "source": [
        "def read_file(data_path):\n",
        "  ''' Load data from train.csv or test.csv. '''\n",
        "\n",
        "  data = pd.read_csv(data_path, sep=';')\n",
        "  for col in ['state', 'n_state', 'action_reward']:\n",
        "    data[col] = [np.array([[np.int32(k) for k in ee.split('&')] for ee in e.split('|')]) for e in data[col]]\n",
        "  for col in ['state', 'n_state']:\n",
        "    data[col] = [np.array([e[0] for e in l]) for l in data[col]]\n",
        "\n",
        "  data['action'] = [[e[0] for e in l] for l in data['action_reward']]\n",
        "  data['reward'] = [tuple(e[1] for e in l) for l in data['action_reward']]\n",
        "  data.drop(columns=['action_reward'], inplace=True)\n",
        "\n",
        "  return data\n",
        "\n",
        "def read_embeddings(embeddings_path):\n",
        "  ''' Load embeddings (a vector for each item). '''\n",
        "\n",
        "  embeddings = pd.read_csv(embeddings_path, sep=';')\n",
        "\n",
        "  return np.array([[np.float64(k) for k in e.split('|')]\n",
        "                   for e in embeddings['vectors']])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGvAXnJayWjt"
      },
      "source": [
        "# Environment/Simulator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H00n2XsouByw"
      },
      "source": [
        "class Environment():\n",
        "  def __init__(self, data, embeddings, alpha, gamma, fixed_length):\n",
        "    self.embeddings = embeddings\n",
        "\n",
        "    self.embedded_data = pd.DataFrame()\n",
        "    # 将 物品id 转换为嵌入向量\n",
        "    self.embedded_data['state'] = [np.array([embeddings.get_embedding(item_id)\n",
        "      for item_id in row['state']]) for _, row in data.iterrows()]\n",
        "    self.embedded_data['action'] = [np.array([embeddings.get_embedding(item_id)\n",
        "      for item_id in row['action']]) for _, row in data.iterrows()]\n",
        "    self.embedded_data['reward'] = data['reward']\n",
        "\n",
        "    self.alpha = alpha # α (alpha) in Equation (1) 用于平衡状态和动作的余弦相似度\n",
        "    self.gamma = gamma # Γ (Gamma) in Equation (4) 用于计算累积奖励\n",
        "    self.fixed_length = fixed_length\n",
        "    self.current_state = self.reset() #初始化当前状态\n",
        "    self.groups = self.get_groups()\n",
        "\n",
        "  def reset(self):\n",
        "    # 重置状态\n",
        "    self.init_state = self.embedded_data['state'].sample(1).values[0]\n",
        "    return self.init_state\n",
        "\n",
        "  def step(self, actions):\n",
        "    '''\n",
        "    Compute reward and update state.\n",
        "    Args:\n",
        "      actions: embedded chosen items. 如果模拟奖励大于 0，则将所选动作添加到当前状态的末尾。\n",
        "    Returns:\n",
        "      cumulated_reward: overall reward.\n",
        "      current_state: updated state.\n",
        "      如果 fixed_length 为 True，则移除当前状态的第一个元素。\n",
        "    '''\n",
        "\n",
        "    # '18: Compute overall reward r_t according to Equation (4)'\n",
        "    simulated_rewards, cumulated_reward = self.simulate_rewards(self.current_state.reshape((1, -1)), actions.reshape((1, -1)))\n",
        "\n",
        "    # '11: Set s_t+1 = s_t' <=> self.current_state = self.current_state\n",
        "    # 可以理解为 推荐的一组物品中 奖励大于0的表示 用户有交互 状态发生了增加\n",
        "    for k in range(len(simulated_rewards)): # '12: for k = 1, K do'\n",
        "      if simulated_rewards[k] > 0: # '13: if r_t^k > 0 then'\n",
        "        # '14: Add a_t^k to the end of s_t+1'\n",
        "        self.current_state = np.append(self.current_state, [actions[k]], axis=0)\n",
        "        if self.fixed_length: # '15: Remove the first item of s_t+1'\n",
        "          self.current_state = np.delete(self.current_state, 0, axis=0)\n",
        "\n",
        "    return cumulated_reward, self.current_state\n",
        "\n",
        "  def get_groups(self):\n",
        "    ''' Calculate average state/action value for each group. Equation (3).\n",
        "    为了解决 当前状态 - 动作对p 与 历史状态 - 动作对m 的余弦相似度 计算复杂的问题，\n",
        "    首先根据奖励对用户的历史浏览记录进行分组。因为奖励的排列组合数量通常是有限的\n",
        "\n",
        "    转化为  当前状态 - 动作对 对属于奖励组U_x的所有历史状态 - 动作对与的余弦相似度求和，再除以与所有属于奖励组集合U的历史状态 - 动作对的余弦相似度之和。\n",
        "     '''\n",
        "\n",
        "    groups = []\n",
        "    for rewards, group in self.embedded_data.groupby(['reward']):\n",
        "      size = group.shape[0]\n",
        "      states = np.array(list(group['state'].values))\n",
        "      actions = np.array(list(group['action'].values))\n",
        "      groups.append({\n",
        "        'size': size, # N_x in article 奖励为U_x的用户历史浏览记录组的大小\n",
        "        'rewards': rewards, # U_x in article (combination of rewards)\n",
        "        'average state': (np.sum(states / np.linalg.norm(states, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)),\n",
        "        # s_x^- 奖励为U_x时的平均状态向量\n",
        "        'average action': (np.sum(actions / np.linalg.norm(actions, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1))\n",
        "        # a_x^- 奖励为U_x时的平均动作向量\n",
        "      })\n",
        "    return groups\n",
        "\n",
        "  def simulate_rewards(self, current_state, chosen_actions, reward_type='grouped cosine'):\n",
        "    '''\n",
        "    Calculate simulated rewards. 模拟奖励过程\n",
        "    Args:\n",
        "      current_state: history, list of embedded items.\n",
        "      chosen_actions: embedded chosen items.\n",
        "      reward_type: from ['normal', 'grouped average', 'grouped cosine'].\n",
        "    Returns:\n",
        "      returned_rewards: most probable rewards.\n",
        "      cumulated_reward: probability weighted rewards.\n",
        "    '''\n",
        "\n",
        "    # Equation (1)\n",
        "    def cosine_state_action(s_t, a_t, s_i, a_i):\n",
        "      cosine_state = np.dot(s_t, s_i.T) / (np.linalg.norm(s_t, 2) * np.linalg.norm(s_i, 2)) # 计算当前状态 s_t 与历史状态 s_i 的余弦相似度\n",
        "      cosine_action = np.dot(a_t, a_i.T) / (np.linalg.norm(a_t, 2) * np.linalg.norm(a_i, 2)) # 计算当前动作 a_t 与历史动作 a_i 的余弦相似度\n",
        "      return (self.alpha * cosine_state + (1 - self.alpha) * cosine_action).reshape((1,))\n",
        "\n",
        "    if reward_type == 'normal':\n",
        "      # Calculate simulated reward in normal way: Equation (2)\n",
        "      # 根据公式 (2) 以普通方式计算概率。遍历 self.embedded_data 中的每一行，计算当前状态 - 动作对与每一行的状态 - 动作对的余弦相似度。\n",
        "      probabilities = [cosine_state_action(current_state, chosen_actions, row['state'], row['action'])\n",
        "        for _, row in self.embedded_data.iterrows()]\n",
        "    elif reward_type == 'grouped average':\n",
        "      # Calculate simulated reward by grouped average: Equation (3)\n",
        "      probabilities = np.array([g['size'] for g in self.groups]) *\\\n",
        "        [(self.alpha * (np.dot(current_state, g['average state'].T) / np.linalg.norm(current_state, 2))\\\n",
        "        + (1 - self.alpha) * (np.dot(chosen_actions, g['average action'].T) / np.linalg.norm(chosen_actions, 2)))\n",
        "        for g in self.groups]\n",
        "    elif reward_type == 'grouped cosine':\n",
        "      # Calculate simulated reward by grouped cosine: Equations (1) and (3)\n",
        "      probabilities = [cosine_state_action(current_state, chosen_actions, g['average state'], g['average action'])\n",
        "        for g in self.groups]\n",
        "\n",
        "    # Normalize (sum to 1)\n",
        "    probabilities = np.array(probabilities) / sum(probabilities)\n",
        "\n",
        "    # Get most probable rewards\n",
        "    if reward_type == 'normal':\n",
        "      # 从 self.embedded_data 中选择概率最大的行，并返回该行的 reward 列的值。\n",
        "      returned_rewards = self.embedded_data.iloc[np.argmax(probabilities)]['reward'] # 复杂度最高 但是最精确？\n",
        "    elif reward_type in ['grouped average', 'grouped cosine']:\n",
        "      # 从 self.groups 中选择概率最大的组，并返回该组的 rewards 值。\n",
        "      returned_rewards = self.groups[np.argmax(probabilities)]['rewards']\n",
        "\n",
        "    # Equation (4)\n",
        "    def overall_reward(rewards, gamma):\n",
        "      return np.sum([gamma**k * reward for k, reward in enumerate(rewards)])\n",
        "\n",
        "    if reward_type in ['normal', 'grouped average']:\n",
        "      # Get cumulated reward: Equation (4) 累积奖励\n",
        "      cumulated_reward = overall_reward(returned_rewards, self.gamma)\n",
        "    elif reward_type == 'grouped cosine':\n",
        "      # Get probability weighted cumulated reward\n",
        "      cumulated_reward = np.sum([p * overall_reward(g['rewards'], self.gamma)\n",
        "        for p, g in zip(probabilities, self.groups)])\n",
        "\n",
        "    return returned_rewards, cumulated_reward"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-rEdckqygGY"
      },
      "source": [
        "# Actor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YK3aSNaTouH"
      },
      "source": [
        "class Actor():\n",
        "  ''' Policy function approximator. '''\n",
        "\n",
        "  def __init__(self, sess, state_space_size, action_space_size, batch_size, ra_length, history_length, embedding_size, tau, learning_rate, scope='actor'):\n",
        "    self.sess = sess\n",
        "    self.state_space_size = state_space_size\n",
        "    self.action_space_size = action_space_size\n",
        "    self.batch_size = batch_size\n",
        "    self.ra_length = ra_length\n",
        "    self.history_length = history_length\n",
        "    self.embedding_size = embedding_size\n",
        "    self.tau = tau\n",
        "    self.learning_rate = learning_rate\n",
        "    self.scope = scope\n",
        "\n",
        "    with tf.variable_scope(self.scope):\n",
        "      # Build Actor network\n",
        "      self.action_weights, self.state, self.sequence_length = self._build_net('estimator_actor')\n",
        "      self.network_params = tf.trainable_variables()\n",
        "\n",
        "      # Build target Actor network\n",
        "      self.target_action_weights, self.target_state, self.target_sequence_length = self._build_net('target_actor')\n",
        "      self.target_network_params = tf.trainable_variables()[len(self.network_params):] # TODO: why sublist [len(x):]? Maybe because its equal to network_params + target_network_params\n",
        "\n",
        "      # Initialize target network weights with network weights (θ^π′ ← θ^π)\n",
        "      self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
        "        for i in range(len(self.target_network_params))]\n",
        "\n",
        "      # Update target network weights (θ^π′ ← τθ^π + (1 − τ)θ^π′)\n",
        "      self.update_target_network_params = [self.target_network_params[i].assign(\n",
        "        tf.multiply(self.tau, self.network_params[i]) +\n",
        "        tf.multiply(1 - self.tau, self.target_network_params[i]))\n",
        "        for i in range(len(self.target_network_params))]\n",
        "\n",
        "      # Gradient computation from Critic's action_gradients\n",
        "      self.action_gradients = tf.placeholder(tf.float32, [None, self.action_space_size])\n",
        "      gradients = tf.gradients(tf.reshape(self.action_weights, [self.batch_size, self.action_space_size], name='42222222222'),\n",
        "                               self.network_params,\n",
        "                               self.action_gradients)\n",
        "      params_gradients = list(map(lambda x: tf.div(x, self.batch_size * self.action_space_size), gradients))\n",
        "\n",
        "      # Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
        "      self.optimizer = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(\n",
        "          zip(params_gradients, self.network_params))\n",
        "\n",
        "  def _build_net(self, scope):\n",
        "    ''' Build the (target) Actor network. '''\n",
        "\n",
        "    def gather_last_output(data, seq_lens):\n",
        "      def cli_value(x, v):\n",
        "        y = tf.constant(v, shape=x.get_shape(), dtype=tf.int64)\n",
        "        x = tf.cast(x, tf.int64)\n",
        "        return tf.where(tf.greater(x, y), x, y)\n",
        "\n",
        "      batch_range = tf.range(tf.cast(tf.shape(data)[0], dtype=tf.int64), dtype=tf.int64)\n",
        "      tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype=tf.int64)\n",
        "      indices = tf.stack([batch_range, tmp_end], axis=1)\n",
        "      return tf.gather_nd(data, indices)\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "      # Inputs: current state, sequence_length\n",
        "      # Outputs: action weights to compute the score Equation (6)\n",
        "      state = tf.placeholder(tf.float32, [None, self.state_space_size], 'state')\n",
        "      state_ = tf.reshape(state, [-1, self.history_length, self.embedding_size])\n",
        "      sequence_length = tf.placeholder(tf.int32, [None], 'sequence_length')\n",
        "      cell = tf.nn.rnn_cell.GRUCell(self.embedding_size,\n",
        "                                    activation=tf.nn.relu,\n",
        "                                    kernel_initializer=tf.initializers.random_normal(),\n",
        "                                    bias_initializer=tf.zeros_initializer())\n",
        "      outputs, _ = tf.nn.dynamic_rnn(cell, state_, dtype=tf.float32, sequence_length=sequence_length)\n",
        "      last_output = gather_last_output(outputs, sequence_length) # TODO: replace by h\n",
        "      x = tf.keras.layers.Dense(self.ra_length * self.embedding_size)(last_output)\n",
        "      action_weights = tf.reshape(x, [-1, self.ra_length, self.embedding_size])\n",
        "\n",
        "    return action_weights, state, sequence_length\n",
        "\n",
        "  def train(self, state, sequence_length, action_gradients):\n",
        "    '''  Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s). '''\n",
        "    self.sess.run(self.optimizer,\n",
        "                  feed_dict={\n",
        "                      self.state: state,\n",
        "                      self.sequence_length: sequence_length,\n",
        "                      self.action_gradients: action_gradients})\n",
        "\n",
        "  def predict(self, state, sequence_length):\n",
        "    return self.sess.run(self.action_weights,\n",
        "                         feed_dict={\n",
        "                             self.state: state,\n",
        "                             self.sequence_length: sequence_length})\n",
        "\n",
        "  def predict_target(self, state, sequence_length):\n",
        "    return self.sess.run(self.target_action_weights,\n",
        "                         feed_dict={\n",
        "                             self.target_state: state,\n",
        "                             self.target_sequence_length: sequence_length})\n",
        "\n",
        "  def init_target_network(self):\n",
        "    self.sess.run(self.init_target_network_params)\n",
        "\n",
        "  def update_target_network(self):\n",
        "    self.sess.run(self.update_target_network_params)\n",
        "\n",
        "  def get_recommendation_list(self, ra_length, noisy_state, embeddings, target=False):\n",
        "    '''\n",
        "    Algorithm 2\n",
        "    Args:\n",
        "      ra_length: length of the recommendation list.\n",
        "      noisy_state: current/remembered environment state with noise.\n",
        "      embeddings: Embeddings object.\n",
        "      target: boolean to use Actor's network or target network.\n",
        "    Returns:\n",
        "      Recommendation List: list of embedded items as future actions.\n",
        "    '''\n",
        "\n",
        "    def get_score(weights, embedding, batch_size):\n",
        "      '''\n",
        "      Equation (6)\n",
        "      Args:\n",
        "        weights: w_t^k shape=(embedding_size,).\n",
        "        embedding: e_i shape=(embedding_size,).\n",
        "      Returns:\n",
        "        score of the item i: score_i=w_t^k.e_i^T shape=(1,).\n",
        "      '''\n",
        "      ret = np.dot(weights, embedding.T)\n",
        "      return ret\n",
        "\n",
        "    batch_size = noisy_state.shape[0]\n",
        "\n",
        "    # '1: Generate w_t = {w_t^1, ..., w_t^K} according to Equation (5)'\n",
        "    method = self.predict_target if target else self.predict\n",
        "    weights = method(noisy_state, [ra_length] * batch_size)\n",
        "\n",
        "    # '3: Score items in I according to Equation (6)'\n",
        "    scores = np.array([[[get_score(weights[i][k], embedding, batch_size)\n",
        "      for embedding in embeddings.get_embedding_vector()]\n",
        "      for k in range(ra_length)]\n",
        "      for i in range(batch_size)])\n",
        "\n",
        "    # '8: return a_t'\n",
        "    return np.array([[embeddings.get_embedding(np.argmax(scores[i][k]))\n",
        "      for k in range(ra_length)]\n",
        "      for i in range(batch_size)])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVAPvO0Nyyy3"
      },
      "source": [
        "# Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj3yEhnqy0Iy"
      },
      "source": [
        "class Critic():\n",
        "  ''' Value function approximator. '''\n",
        "\n",
        "  def __init__(self, sess, state_space_size, action_space_size, history_length, embedding_size, tau, learning_rate, scope='critic'):\n",
        "    self.sess = sess\n",
        "    self.state_space_size = state_space_size\n",
        "    self.action_space_size = action_space_size\n",
        "    self.history_length = history_length\n",
        "    self.embedding_size = embedding_size\n",
        "    self.tau = tau\n",
        "    self.learning_rate = learning_rate\n",
        "    self.scope = scope\n",
        "\n",
        "    with tf.variable_scope(self.scope):\n",
        "      # Build Critic network\n",
        "      self.critic_Q_value, self.state, self.action, self.sequence_length = self._build_net('estimator_critic')\n",
        "      self.network_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='estimator_critic')\n",
        "\n",
        "      # Build target Critic network\n",
        "      self.target_Q_value, self.target_state, self.target_action, self.target_sequence_length = self._build_net('target_critic')\n",
        "      self.target_network_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='target_critic')\n",
        "\n",
        "      # Initialize target network weights with network weights (θ^µ′ ← θ^µ)\n",
        "      self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
        "        for i in range(len(self.target_network_params))]\n",
        "\n",
        "      # Update target network weights (θ^µ′ ← τθ^µ + (1 − τ)θ^µ′)\n",
        "      self.update_target_network_params = [self.target_network_params[i].assign(\n",
        "        tf.multiply(self.tau, self.network_params[i]) +\n",
        "        tf.multiply(1 - self.tau, self.target_network_params[i]))\n",
        "        for i in range(len(self.target_network_params))]\n",
        "\n",
        "      # Minimize MSE between Critic's and target Critic's outputed Q-values\n",
        "      self.expected_reward = tf.placeholder(tf.float32, [None, 1])\n",
        "      self.loss = tf.reduce_mean(tf.squared_difference(self.expected_reward, self.critic_Q_value))\n",
        "      self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
        "\n",
        "      # Compute ∇_a.Q(s, a|θ^µ)\n",
        "      self.action_gradients = tf.gradients(self.critic_Q_value, self.action)\n",
        "\n",
        "  def _build_net(self, scope):\n",
        "    ''' Build the (target) Critic network. '''\n",
        "\n",
        "    def gather_last_output(data, seq_lens):\n",
        "      def cli_value(x, v):\n",
        "        y = tf.constant(v, shape=x.get_shape(), dtype=tf.int64)\n",
        "        return tf.where(tf.greater(x, y), x, y)\n",
        "\n",
        "      this_range = tf.range(tf.cast(tf.shape(seq_lens)[0], dtype=tf.int64), dtype=tf.int64)\n",
        "      tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype=tf.int64)\n",
        "      indices = tf.stack([this_range, tmp_end], axis=1)\n",
        "      return tf.gather_nd(data, indices)\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "      # Inputs: current state, current action\n",
        "      # Outputs: predicted Q-value\n",
        "      state = tf.placeholder(tf.float32, [None, self.state_space_size], 'state')\n",
        "      state_ = tf.reshape(state, [-1, self.history_length, self.embedding_size])\n",
        "      action = tf.placeholder(tf.float32, [None, self.action_space_size], 'action')\n",
        "      sequence_length = tf.placeholder(tf.int64, [None], name='critic_sequence_length')\n",
        "      cell = tf.nn.rnn_cell.GRUCell(self.history_length,\n",
        "                                    activation=tf.nn.relu,\n",
        "                                    kernel_initializer=tf.initializers.random_normal(),\n",
        "                                    bias_initializer=tf.zeros_initializer())\n",
        "      predicted_state, _ = tf.nn.dynamic_rnn(cell, state_, dtype=tf.float32, sequence_length=sequence_length)\n",
        "      predicted_state = gather_last_output(predicted_state, sequence_length)\n",
        "\n",
        "      inputs = tf.concat([predicted_state, action], axis=-1)\n",
        "      layer1 = tf.layers.Dense(32, activation=tf.nn.relu)(inputs)\n",
        "      layer2 = tf.layers.Dense(16, activation=tf.nn.relu)(layer1)\n",
        "      critic_Q_value = tf.layers.Dense(1)(layer2)\n",
        "      return critic_Q_value, state, action, sequence_length\n",
        "\n",
        "  def train(self, state, action, sequence_length, expected_reward):\n",
        "    ''' Minimize MSE between expected reward and target Critic's Q-value. '''\n",
        "    return self.sess.run([self.critic_Q_value, self.loss, self.optimizer],\n",
        "                         feed_dict={\n",
        "                             self.state: state,\n",
        "                             self.action: action,\n",
        "                             self.sequence_length: sequence_length,\n",
        "                             self.expected_reward: expected_reward})\n",
        "\n",
        "  def predict(self, state, action, sequence_length):\n",
        "    ''' Returns Critic's predicted Q-value. '''\n",
        "    return self.sess.run(self.critic_Q_value,\n",
        "                         feed_dict={\n",
        "                             self.state: state,\n",
        "                             self.action: action,\n",
        "                             self.sequence_length: sequence_length})\n",
        "\n",
        "  def predict_target(self, state, action, sequence_length):\n",
        "    ''' Returns target Critic's predicted Q-value. '''\n",
        "    return self.sess.run(self.target_Q_value,\n",
        "                         feed_dict={\n",
        "                             self.target_state: state,\n",
        "                             self.target_action: action,\n",
        "                             self.target_sequence_length: sequence_length})\n",
        "\n",
        "  def get_action_gradients(self, state, action, sequence_length):\n",
        "    ''' Returns ∇_a.Q(s, a|θ^µ). '''\n",
        "    return np.array(self.sess.run(self.action_gradients,\n",
        "                         feed_dict={\n",
        "                             self.state: state,\n",
        "                             self.action: action,\n",
        "                             self.sequence_length: sequence_length})[0])\n",
        "\n",
        "  def init_target_network(self):\n",
        "    self.sess.run(self.init_target_network_params)\n",
        "\n",
        "  def update_target_network(self):\n",
        "    self.sess.run(self.update_target_network_params)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNP3jdofyh7M"
      },
      "source": [
        "# Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvIqtE2YzoUa"
      },
      "source": [
        "class ReplayMemory():\n",
        "  ''' Replay memory D in article. '''\n",
        "\n",
        "  def __init__(self, buffer_size):\n",
        "    self.buffer_size = buffer_size\n",
        "    # self.buffer = [[row['state'], row['action'], row['reward'], row['n_state']] for _, row in data.iterrows()][-self.buffer_size:] TODO: empty or not?\n",
        "    self.buffer = []\n",
        "\n",
        "  def add(self, state, action, reward, n_state):\n",
        "    self.buffer.append([state, action, reward, n_state])\n",
        "    if len(self.buffer) > self.buffer_size:\n",
        "      self.buffer.pop(0)\n",
        "\n",
        "  def size(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def sample_batch(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x-ekweNfStD"
      },
      "source": [
        "def experience_replay(replay_memory, batch_size, actor, critic, embeddings, ra_length, state_space_size, action_space_size, discount_factor):\n",
        "  '''\n",
        "  Experience replay.\n",
        "  Args:\n",
        "    replay_memory: replay memory D in article.\n",
        "    batch_size: sample size.\n",
        "    actor: Actor network.\n",
        "    critic: Critic network.\n",
        "    embeddings: Embeddings object.\n",
        "    state_space_size: dimension of states.\n",
        "    action_space_size: dimensions of actions.\n",
        "  Returns:\n",
        "    Best Q-value, loss of Critic network for printing/recording purpose.\n",
        "  '''\n",
        "\n",
        "  # '22: Sample minibatch of N transitions (s, a, r, s′) from D'\n",
        "  samples = replay_memory.sample_batch(batch_size)\n",
        "  states = np.array([s[0] for s in samples])\n",
        "  actions = np.array([s[1] for s in samples])\n",
        "  rewards = np.array([s[2] for s in samples])\n",
        "  n_states = np.array([s[3] for s in samples]).reshape(-1, state_space_size)\n",
        "\n",
        "  # '23: Generate a′ by target Actor network according to Algorithm 2'\n",
        "  n_actions = actor.get_recommendation_list(ra_length, states, embeddings, target=True).reshape(-1, action_space_size)\n",
        "\n",
        "  # Calculate predicted Q′(s′, a′|θ^µ′) value\n",
        "  target_Q_value = critic.predict_target(n_states, n_actions, [ra_length] * batch_size)\n",
        "\n",
        "  # '24: Set y = r + γQ′(s′, a′|θ^µ′)'\n",
        "  expected_rewards = rewards + discount_factor * target_Q_value\n",
        "\n",
        "  # '25: Update Critic by minimizing (y − Q(s, a|θ^µ))²'\n",
        "  critic_Q_value, critic_loss, _ = critic.train(states, actions, [ra_length] * batch_size, expected_rewards)\n",
        "\n",
        "  # '26: Update the Actor using the sampled policy gradient'\n",
        "  action_gradients = critic.get_action_gradients(states, n_actions, [ra_length] * batch_size)\n",
        "  actor.train(states, [ra_length] * batch_size, action_gradients)\n",
        "\n",
        "  # '27: Update the Critic target networks'\n",
        "  critic.update_target_network()\n",
        "\n",
        "  # '28: Update the Actor target network'\n",
        "  actor.update_target_network()\n",
        "\n",
        "  return np.amax(critic_Q_value), critic_loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzYw4cAhzYDF"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRDXsrsMy69k"
      },
      "source": [
        "class OrnsteinUhlenbeckNoise:\n",
        "  ''' Noise for Actor predictions. '''\n",
        "  def __init__(self, action_space_size, mu=0, theta=0.5, sigma=0.2):\n",
        "    self.action_space_size = action_space_size\n",
        "    self.mu = mu\n",
        "    self.theta = theta\n",
        "    self.sigma = sigma\n",
        "    self.state = np.ones(self.action_space_size) * self.mu\n",
        "\n",
        "  def get(self):\n",
        "    self.state += self.theta * (self.mu - self.state) + self.sigma * np.random.rand(self.action_space_size)\n",
        "    return self.state\n",
        "\n",
        "def train(sess, environment, actor, critic, embeddings, history_length, ra_length, buffer_size, batch_size, discount_factor, nb_episodes, filename_summary):\n",
        "  ''' Algorithm 3 in article. '''\n",
        "\n",
        "  # Set up summary operators\n",
        "  def build_summaries():\n",
        "    episode_reward = tf.Variable(0.)\n",
        "    tf.summary.scalar('reward', episode_reward)\n",
        "    episode_max_Q = tf.Variable(0.)\n",
        "    tf.summary.scalar('max_Q_value', episode_max_Q)\n",
        "    critic_loss = tf.Variable(0.)\n",
        "    tf.summary.scalar('critic_loss', critic_loss)\n",
        "\n",
        "    summary_vars = [episode_reward, episode_max_Q, critic_loss]\n",
        "    summary_ops = tf.summary.merge_all()\n",
        "    return summary_ops, summary_vars\n",
        "\n",
        "  summary_ops, summary_vars = build_summaries()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  writer = tf.summary.FileWriter(filename_summary, sess.graph)\n",
        "\n",
        "  # '2: Initialize target network f′ and Q′'\n",
        "  actor.init_target_network()\n",
        "  critic.init_target_network()\n",
        "\n",
        "  # '3: Initialize the capacity of replay memory D'\n",
        "  replay_memory = ReplayMemory(buffer_size) # Memory D in article\n",
        "  replay = False\n",
        "\n",
        "\n",
        "  start_time = time.time()\n",
        "  for i_session in range(nb_episodes): # '4: for session = 1, M do'\n",
        "    session_reward = 0\n",
        "    session_Q_value = 0\n",
        "    session_critic_loss = 0\n",
        "\n",
        "    # '5: Reset the item space I' is useless because unchanged.\n",
        "\n",
        "    states = environment.reset() # '6: Initialize state s_0 from previous sessions'\n",
        "\n",
        "    if (i_session + 1) % 10 == 0: # Update average parameters every 10 episodes\n",
        "      environment.groups = environment.get_groups()\n",
        "\n",
        "    exploration_noise = OrnsteinUhlenbeckNoise(history_length * embeddings.size())\n",
        "\n",
        "    for t in range(nb_rounds): # '7: for t = 1, T do'\n",
        "      # '8: Stage 1: Transition Generating Stage'\n",
        "\n",
        "      # '9: Select an action a_t = {a_t^1, ..., a_t^K} according to Algorithm 2'\n",
        "      actions = actor.get_recommendation_list(\n",
        "          ra_length,\n",
        "          states.reshape(1, -1), # TODO + exploration_noise.get().reshape(1, -1),\n",
        "          embeddings).reshape(ra_length, embeddings.size())\n",
        "\n",
        "      # '10: Execute action a_t and observe the reward list {r_t^1, ..., r_t^K} for each item in a_t'\n",
        "      rewards, next_states = environment.step(actions)\n",
        "\n",
        "      # '19: Store transition (s_t, a_t, r_t, s_t+1) in D'\n",
        "      replay_memory.add(states.reshape(history_length * embeddings.size()),\n",
        "                        actions.reshape(ra_length * embeddings.size()),\n",
        "                        [rewards],\n",
        "                        next_states.reshape(history_length * embeddings.size()))\n",
        "\n",
        "      states = next_states # '20: Set s_t = s_t+1'\n",
        "\n",
        "      session_reward += rewards\n",
        "\n",
        "      # '21: Stage 2: Parameter Updating Stage'\n",
        "      if replay_memory.size() >= batch_size: # Experience replay\n",
        "        replay = True\n",
        "        replay_Q_value, critic_loss = experience_replay(replay_memory, batch_size,\n",
        "          actor, critic, embeddings, ra_length, history_length * embeddings.size(),\n",
        "          ra_length * embeddings.size(), discount_factor)\n",
        "        session_Q_value += replay_Q_value\n",
        "        session_critic_loss += critic_loss\n",
        "\n",
        "      summary_str = sess.run(summary_ops,\n",
        "                             feed_dict={summary_vars[0]: session_reward,\n",
        "                                        summary_vars[1]: session_Q_value,\n",
        "                                        summary_vars[2]: session_critic_loss})\n",
        "\n",
        "      writer.add_summary(summary_str, i_session)\n",
        "\n",
        "      '''\n",
        "      print(state_to_items(embeddings.embed(data['state'][0]), actor, ra_length, embeddings),\n",
        "            state_to_items(embeddings.embed(data['state'][0]), actor, ra_length, embeddings, True))\n",
        "      '''\n",
        "\n",
        "    str_loss = str('Loss=%0.4f' % session_critic_loss)\n",
        "    print(('Episode %d/%d Reward=%d Time=%ds ' + (str_loss if replay else 'No replay')) % (i_session + 1, nb_episodes, session_reward, time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "\n",
        "  writer.close()\n",
        "  tf.train.Saver().save(sess, 'models.h5', write_meta_graph=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx_5zk2hll5e"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8vv6bRi5_ro"
      },
      "source": [
        "# Hyperparameters\n",
        "history_length = 12 # N in article\n",
        "ra_length = 4 # K in article\n",
        "discount_factor = 0.99 # Gamma in Bellman equation\n",
        "actor_lr = 0.0001\n",
        "critic_lr = 0.001\n",
        "tau = 0.001 # τ in Algorithm 3\n",
        "batch_size = 64\n",
        "nb_episodes = 100\n",
        "nb_rounds = 50\n",
        "filename_summary = 'summary.txt'\n",
        "alpha = 0.5 # α (alpha) in Equation (1)\n",
        "gamma = 0.9 # Γ (Gamma) in Equation (4)\n",
        "buffer_size = 1000000 # Size of replay memory D in article\n",
        "fixed_length = True # Fixed memory length\n",
        "\n",
        "dg = DataGenerator('ml-100k/u.data', 'ml-100k/u.item')\n",
        "dg.gen_train_test(0.8, seed=42)\n",
        "\n",
        "dg.write_csv('train.csv', dg.train, nb_states=[history_length], nb_actions=[ra_length])\n",
        "dg.write_csv('test.csv', dg.test, nb_states=[history_length], nb_actions=[ra_length])\n",
        "\n",
        "data = read_file('train.csv')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-aa79rvlpWH"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezsuxZbllgDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7640d561-91f9-40db-a309-b15b9f3e441e"
      },
      "source": [
        "if True: # Generate embeddings?\n",
        "  eg = EmbeddingsGenerator(dg.user_train, pd.read_csv('ml-100k/u.data', sep='\\t', names=['userId', 'itemId', 'rating', 'timestamp']))\n",
        "  eg.train(nb_epochs=300)\n",
        "  # eg.train(nb_epochs=1)\n",
        "  train_loss, train_accuracy = eg.test(dg.user_train)\n",
        "  print('Train set: Loss=%.4f ; Accuracy=%.1f%%' % (train_loss, train_accuracy * 100))\n",
        "  test_loss, test_accuracy = eg.test(dg.user_test)\n",
        "  print('Test set: Loss=%.4f ; Accuracy=%.1f%%' % (test_loss, test_accuracy * 100))\n",
        "  eg.save_embeddings('embeddings.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 50ms/step - accuracy: 0.0067 - loss: 7.1570 - val_accuracy: 0.0116 - val_loss: 6.5334\n",
            "2/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.0141 - loss: 6.4574 - val_accuracy: 0.0158 - val_loss: 6.3426\n",
            "3/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.0153 - loss: 6.3147 - val_accuracy: 0.0138 - val_loss: 6.1949\n",
            "4/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.0113 - loss: 6.2410 - val_accuracy: 0.0174 - val_loss: 6.1925\n",
            "5/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.0170 - loss: 6.1872 - val_accuracy: 0.0198 - val_loss: 6.0678\n",
            "6/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.0177 - loss: 6.1184 - val_accuracy: 0.0218 - val_loss: 6.0414\n",
            "7/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.0172 - loss: 6.0990 - val_accuracy: 0.0194 - val_loss: 5.9796\n",
            "8/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 79ms/step - accuracy: 0.0188 - loss: 6.0397 - val_accuracy: 0.0210 - val_loss: 5.9715\n",
            "9/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 71ms/step - accuracy: 0.0184 - loss: 5.9561 - val_accuracy: 0.0232 - val_loss: 5.9660\n",
            "10/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.0202 - loss: 5.9948 - val_accuracy: 0.0254 - val_loss: 5.8985\n",
            "11/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.0239 - loss: 5.9242 - val_accuracy: 0.0316 - val_loss: 5.9059\n",
            "12/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.0297 - loss: 5.9487 - val_accuracy: 0.0248 - val_loss: 5.9401\n",
            "13/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.0273 - loss: 5.9157 - val_accuracy: 0.0318 - val_loss: 5.8379\n",
            "14/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.0260 - loss: 5.9398 - val_accuracy: 0.0322 - val_loss: 5.8229\n",
            "15/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.0323 - loss: 5.9094 - val_accuracy: 0.0426 - val_loss: 5.7901\n",
            "16/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.0363 - loss: 5.8491 - val_accuracy: 0.0418 - val_loss: 5.7873\n",
            "17/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.0409 - loss: 5.8077 - val_accuracy: 0.0424 - val_loss: 5.7013\n",
            "18/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.0407 - loss: 5.7637 - val_accuracy: 0.0468 - val_loss: 5.7228\n",
            "19/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.0426 - loss: 5.7669 - val_accuracy: 0.0446 - val_loss: 5.6946\n",
            "20/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - accuracy: 0.0390 - loss: 5.7188 - val_accuracy: 0.0472 - val_loss: 5.7114\n",
            "21/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.0525 - loss: 5.6972 - val_accuracy: 0.0560 - val_loss: 5.6399\n",
            "22/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.0420 - loss: 5.7548 - val_accuracy: 0.0616 - val_loss: 5.6073\n",
            "23/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.0613 - loss: 5.6777 - val_accuracy: 0.0610 - val_loss: 5.5846\n",
            "24/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.0528 - loss: 5.6181 - val_accuracy: 0.0688 - val_loss: 5.5184\n",
            "25/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.0562 - loss: 5.6399 - val_accuracy: 0.0668 - val_loss: 5.5052\n",
            "26/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.0603 - loss: 5.5874 - val_accuracy: 0.0748 - val_loss: 5.5177\n",
            "27/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.0652 - loss: 5.5540 - val_accuracy: 0.0760 - val_loss: 5.4994\n",
            "28/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.0659 - loss: 5.5408 - val_accuracy: 0.0812 - val_loss: 5.4620\n",
            "29/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.0730 - loss: 5.5244 - val_accuracy: 0.0898 - val_loss: 5.4438\n",
            "30/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.0776 - loss: 5.5016 - val_accuracy: 0.0912 - val_loss: 5.3798\n",
            "31/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.0845 - loss: 5.4426 - val_accuracy: 0.0986 - val_loss: 5.3737\n",
            "32/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.0820 - loss: 5.4699 - val_accuracy: 0.1000 - val_loss: 5.3361\n",
            "33/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.0849 - loss: 5.4590 - val_accuracy: 0.1146 - val_loss: 5.2467\n",
            "34/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.0866 - loss: 5.4350 - val_accuracy: 0.1112 - val_loss: 5.2712\n",
            "35/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.0948 - loss: 5.4019 - val_accuracy: 0.1158 - val_loss: 5.2172\n",
            "36/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.0905 - loss: 5.3855 - val_accuracy: 0.1288 - val_loss: 5.2065\n",
            "37/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.1014 - loss: 5.3054 - val_accuracy: 0.1272 - val_loss: 5.1175\n",
            "38/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.1137 - loss: 5.2357 - val_accuracy: 0.1328 - val_loss: 5.1421\n",
            "39/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.1109 - loss: 5.2100 - val_accuracy: 0.1364 - val_loss: 5.0929\n",
            "40/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.1176 - loss: 5.2043 - val_accuracy: 0.1366 - val_loss: 5.0777\n",
            "41/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.1276 - loss: 5.1156 - val_accuracy: 0.1644 - val_loss: 4.9777\n",
            "42/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.1195 - loss: 5.1856 - val_accuracy: 0.1558 - val_loss: 4.9681\n",
            "43/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.1317 - loss: 5.0905 - val_accuracy: 0.1532 - val_loss: 4.9769\n",
            "44/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.1358 - loss: 5.0648 - val_accuracy: 0.1760 - val_loss: 4.8750\n",
            "45/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.1511 - loss: 4.9466 - val_accuracy: 0.1664 - val_loss: 4.8813\n",
            "46/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.1402 - loss: 5.0742 - val_accuracy: 0.1904 - val_loss: 4.8317\n",
            "47/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.1463 - loss: 4.9666 - val_accuracy: 0.1828 - val_loss: 4.8838\n",
            "48/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.1501 - loss: 4.9435 - val_accuracy: 0.2012 - val_loss: 4.8329\n",
            "49/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.1625 - loss: 4.8992 - val_accuracy: 0.1952 - val_loss: 4.7319\n",
            "50/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.1696 - loss: 4.8708 - val_accuracy: 0.2012 - val_loss: 4.6870\n",
            "51/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.1686 - loss: 4.8284 - val_accuracy: 0.2198 - val_loss: 4.6440\n",
            "52/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - accuracy: 0.1738 - loss: 4.8031 - val_accuracy: 0.2242 - val_loss: 4.6353\n",
            "53/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1883 - loss: 4.7413 - val_accuracy: 0.2244 - val_loss: 4.5812\n",
            "54/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.1887 - loss: 4.7324 - val_accuracy: 0.2406 - val_loss: 4.5399\n",
            "55/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.1938 - loss: 4.7056 - val_accuracy: 0.2576 - val_loss: 4.4586\n",
            "56/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.1935 - loss: 4.7143 - val_accuracy: 0.2582 - val_loss: 4.4378\n",
            "57/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2224 - loss: 4.5316 - val_accuracy: 0.2724 - val_loss: 4.3742\n",
            "58/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.2139 - loss: 4.5587 - val_accuracy: 0.2624 - val_loss: 4.4123\n",
            "59/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.2120 - loss: 4.5037 - val_accuracy: 0.2844 - val_loss: 4.3346\n",
            "60/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.2405 - loss: 4.4635 - val_accuracy: 0.2822 - val_loss: 4.2877\n",
            "61/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.2443 - loss: 4.4019 - val_accuracy: 0.2986 - val_loss: 4.2205\n",
            "62/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2281 - loss: 4.4540 - val_accuracy: 0.3108 - val_loss: 4.1806\n",
            "63/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.2363 - loss: 4.4161 - val_accuracy: 0.3128 - val_loss: 4.1492\n",
            "64/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.2533 - loss: 4.3417 - val_accuracy: 0.3270 - val_loss: 4.0966\n",
            "65/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.2555 - loss: 4.3565 - val_accuracy: 0.3302 - val_loss: 4.0476\n",
            "66/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.2707 - loss: 4.2784 - val_accuracy: 0.3258 - val_loss: 4.0093\n",
            "67/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.2731 - loss: 4.1891 - val_accuracy: 0.3452 - val_loss: 3.9814\n",
            "68/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.2887 - loss: 4.1603 - val_accuracy: 0.3500 - val_loss: 3.9490\n",
            "69/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.2920 - loss: 4.1709 - val_accuracy: 0.3722 - val_loss: 3.8520\n",
            "70/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2978 - loss: 4.0071 - val_accuracy: 0.3720 - val_loss: 3.8600\n",
            "71/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.3060 - loss: 4.0303 - val_accuracy: 0.3880 - val_loss: 3.7603\n",
            "72/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.3052 - loss: 3.9986 - val_accuracy: 0.3856 - val_loss: 3.7101\n",
            "73/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.3307 - loss: 3.9136 - val_accuracy: 0.3986 - val_loss: 3.6607\n",
            "74/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.3275 - loss: 3.8442 - val_accuracy: 0.4034 - val_loss: 3.6465\n",
            "75/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.3310 - loss: 3.8685 - val_accuracy: 0.4250 - val_loss: 3.5758\n",
            "76/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.3475 - loss: 3.7777 - val_accuracy: 0.4364 - val_loss: 3.5150\n",
            "77/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.3488 - loss: 3.8053 - val_accuracy: 0.4548 - val_loss: 3.4577\n",
            "78/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.3770 - loss: 3.6205 - val_accuracy: 0.4478 - val_loss: 3.4436\n",
            "79/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.3742 - loss: 3.6074 - val_accuracy: 0.4574 - val_loss: 3.4250\n",
            "80/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.3705 - loss: 3.6772 - val_accuracy: 0.4640 - val_loss: 3.3530\n",
            "81/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.3548 - loss: 3.6419 - val_accuracy: 0.4780 - val_loss: 3.2905\n",
            "82/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.3986 - loss: 3.5385 - val_accuracy: 0.4838 - val_loss: 3.2227\n",
            "83/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.4070 - loss: 3.4311 - val_accuracy: 0.5012 - val_loss: 3.2088\n",
            "84/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.4245 - loss: 3.3496 - val_accuracy: 0.5072 - val_loss: 3.1423\n",
            "85/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.4347 - loss: 3.3418 - val_accuracy: 0.5124 - val_loss: 3.1322\n",
            "86/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.4333 - loss: 3.3431 - val_accuracy: 0.5262 - val_loss: 3.0843\n",
            "87/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.4425 - loss: 3.2656 - val_accuracy: 0.5288 - val_loss: 3.0565\n",
            "88/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.4531 - loss: 3.1962 - val_accuracy: 0.5508 - val_loss: 2.9660\n",
            "89/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.4768 - loss: 3.1086 - val_accuracy: 0.5580 - val_loss: 2.8935\n",
            "90/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.4813 - loss: 3.0455 - val_accuracy: 0.5658 - val_loss: 2.8314\n",
            "91/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.4668 - loss: 3.1205 - val_accuracy: 0.5728 - val_loss: 2.8366\n",
            "92/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.4791 - loss: 3.0278 - val_accuracy: 0.5852 - val_loss: 2.7533\n",
            "93/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.4909 - loss: 2.9908 - val_accuracy: 0.5874 - val_loss: 2.7490\n",
            "94/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - accuracy: 0.4946 - loss: 2.9560 - val_accuracy: 0.6118 - val_loss: 2.6627\n",
            "95/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.5096 - loss: 2.9193 - val_accuracy: 0.6152 - val_loss: 2.6048\n",
            "96/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.5198 - loss: 2.8741 - val_accuracy: 0.6142 - val_loss: 2.5850\n",
            "97/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.5338 - loss: 2.7787 - val_accuracy: 0.6308 - val_loss: 2.5164\n",
            "98/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.5493 - loss: 2.7421 - val_accuracy: 0.6334 - val_loss: 2.4947\n",
            "99/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.5548 - loss: 2.6800 - val_accuracy: 0.6318 - val_loss: 2.4931\n",
            "100/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.5543 - loss: 2.7205 - val_accuracy: 0.6600 - val_loss: 2.4156\n",
            "101/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - accuracy: 0.5761 - loss: 2.5688 - val_accuracy: 0.6630 - val_loss: 2.3592\n",
            "102/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.5685 - loss: 2.5787 - val_accuracy: 0.6736 - val_loss: 2.3067\n",
            "103/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.5737 - loss: 2.5796 - val_accuracy: 0.6688 - val_loss: 2.3217\n",
            "104/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.5642 - loss: 2.6317 - val_accuracy: 0.6772 - val_loss: 2.2243\n",
            "105/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.5923 - loss: 2.4334 - val_accuracy: 0.7070 - val_loss: 2.1000\n",
            "106/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.6211 - loss: 2.3346 - val_accuracy: 0.6928 - val_loss: 2.1274\n",
            "107/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.5899 - loss: 2.4611 - val_accuracy: 0.7012 - val_loss: 2.1264\n",
            "108/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.6231 - loss: 2.3027 - val_accuracy: 0.7160 - val_loss: 2.0529\n",
            "109/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.6340 - loss: 2.2548 - val_accuracy: 0.7238 - val_loss: 1.9948\n",
            "110/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.6252 - loss: 2.3257 - val_accuracy: 0.7212 - val_loss: 1.9788\n",
            "111/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.6420 - loss: 2.1695 - val_accuracy: 0.7318 - val_loss: 1.9392\n",
            "112/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.6473 - loss: 2.1921 - val_accuracy: 0.7274 - val_loss: 1.9023\n",
            "113/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.6664 - loss: 2.0734 - val_accuracy: 0.7542 - val_loss: 1.8079\n",
            "114/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.6755 - loss: 2.0498 - val_accuracy: 0.7628 - val_loss: 1.7735\n",
            "115/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.6705 - loss: 2.0600 - val_accuracy: 0.7646 - val_loss: 1.7607\n",
            "116/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.6823 - loss: 2.0310 - val_accuracy: 0.7676 - val_loss: 1.7517\n",
            "117/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.6914 - loss: 1.9380 - val_accuracy: 0.7774 - val_loss: 1.6690\n",
            "118/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.6913 - loss: 1.9571 - val_accuracy: 0.7888 - val_loss: 1.6530\n",
            "119/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.6996 - loss: 1.9057 - val_accuracy: 0.7920 - val_loss: 1.5986\n",
            "120/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.7236 - loss: 1.7578 - val_accuracy: 0.7936 - val_loss: 1.5761\n",
            "121/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - accuracy: 0.7218 - loss: 1.7901 - val_accuracy: 0.7934 - val_loss: 1.6237\n",
            "122/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.7055 - loss: 1.8629 - val_accuracy: 0.7982 - val_loss: 1.5155\n",
            "123/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.7234 - loss: 1.7672 - val_accuracy: 0.8014 - val_loss: 1.5133\n",
            "124/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.7374 - loss: 1.7789 - val_accuracy: 0.8104 - val_loss: 1.4468\n",
            "125/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.7473 - loss: 1.6438 - val_accuracy: 0.8196 - val_loss: 1.4511\n",
            "126/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.7337 - loss: 1.6877 - val_accuracy: 0.8102 - val_loss: 1.4700\n",
            "127/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.7456 - loss: 1.7049 - val_accuracy: 0.8310 - val_loss: 1.3559\n",
            "128/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.7492 - loss: 1.6312 - val_accuracy: 0.8274 - val_loss: 1.3571\n",
            "129/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.7547 - loss: 1.5535 - val_accuracy: 0.8292 - val_loss: 1.3446\n",
            "130/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.7665 - loss: 1.5370 - val_accuracy: 0.8368 - val_loss: 1.3054\n",
            "131/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.7632 - loss: 1.5399 - val_accuracy: 0.8474 - val_loss: 1.2879\n",
            "132/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.7696 - loss: 1.5495 - val_accuracy: 0.8482 - val_loss: 1.2194\n",
            "133/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.7899 - loss: 1.4226 - val_accuracy: 0.8508 - val_loss: 1.1937\n",
            "134/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.7933 - loss: 1.3959 - val_accuracy: 0.8626 - val_loss: 1.1242\n",
            "135/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.7924 - loss: 1.4118 - val_accuracy: 0.8532 - val_loss: 1.1550\n",
            "136/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.7959 - loss: 1.3775 - val_accuracy: 0.8494 - val_loss: 1.1780\n",
            "137/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.7956 - loss: 1.3673 - val_accuracy: 0.8694 - val_loss: 1.1030\n",
            "138/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.8145 - loss: 1.2962 - val_accuracy: 0.8774 - val_loss: 1.0196\n",
            "139/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.8145 - loss: 1.2297 - val_accuracy: 0.8704 - val_loss: 1.0279\n",
            "140/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.8182 - loss: 1.2443 - val_accuracy: 0.8668 - val_loss: 1.0516\n",
            "141/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.8128 - loss: 1.3088 - val_accuracy: 0.8742 - val_loss: 0.9881\n",
            "142/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.8169 - loss: 1.2782 - val_accuracy: 0.8804 - val_loss: 0.9667\n",
            "143/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.8330 - loss: 1.1429 - val_accuracy: 0.8728 - val_loss: 1.0370\n",
            "144/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.8247 - loss: 1.2308 - val_accuracy: 0.8836 - val_loss: 0.9480\n",
            "145/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.8339 - loss: 1.1481 - val_accuracy: 0.8832 - val_loss: 0.9098\n",
            "146/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - accuracy: 0.8439 - loss: 1.0919 - val_accuracy: 0.8884 - val_loss: 0.9098\n",
            "147/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.8313 - loss: 1.1578 - val_accuracy: 0.8982 - val_loss: 0.8665\n",
            "148/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.8553 - loss: 1.0399 - val_accuracy: 0.8990 - val_loss: 0.8484\n",
            "149/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.8501 - loss: 1.0793 - val_accuracy: 0.8950 - val_loss: 0.8709\n",
            "150/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.8446 - loss: 1.1179 - val_accuracy: 0.8900 - val_loss: 0.8833\n",
            "151/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.8427 - loss: 1.1023 - val_accuracy: 0.8968 - val_loss: 0.8471\n",
            "152/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.8500 - loss: 1.0520 - val_accuracy: 0.9030 - val_loss: 0.8045\n",
            "153/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.8505 - loss: 1.0943 - val_accuracy: 0.8932 - val_loss: 0.8203\n",
            "154/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.8666 - loss: 0.9467 - val_accuracy: 0.9074 - val_loss: 0.7507\n",
            "155/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.8566 - loss: 0.9933 - val_accuracy: 0.9020 - val_loss: 0.8167\n",
            "156/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - accuracy: 0.8603 - loss: 0.9587 - val_accuracy: 0.9092 - val_loss: 0.7471\n",
            "157/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.8730 - loss: 0.9220 - val_accuracy: 0.9134 - val_loss: 0.7070\n",
            "158/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.8691 - loss: 0.9212 - val_accuracy: 0.9202 - val_loss: 0.6902\n",
            "159/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.8719 - loss: 0.9542 - val_accuracy: 0.9100 - val_loss: 0.7097\n",
            "160/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.8743 - loss: 0.8693 - val_accuracy: 0.9156 - val_loss: 0.7244\n",
            "161/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.8725 - loss: 0.9172 - val_accuracy: 0.9148 - val_loss: 0.7108\n",
            "162/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.8811 - loss: 0.8611 - val_accuracy: 0.9178 - val_loss: 0.6686\n",
            "163/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - accuracy: 0.8842 - loss: 0.8799 - val_accuracy: 0.9052 - val_loss: 0.7054\n",
            "164/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.8717 - loss: 0.9110 - val_accuracy: 0.9188 - val_loss: 0.6897\n",
            "165/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.8658 - loss: 1.0042 - val_accuracy: 0.9220 - val_loss: 0.6242\n",
            "166/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.8737 - loss: 0.8867 - val_accuracy: 0.9102 - val_loss: 0.6997\n",
            "167/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.8942 - loss: 0.8170 - val_accuracy: 0.9220 - val_loss: 0.6395\n",
            "168/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.8889 - loss: 0.7851 - val_accuracy: 0.9240 - val_loss: 0.6232\n",
            "169/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.8962 - loss: 0.7432 - val_accuracy: 0.9254 - val_loss: 0.5756\n",
            "170/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.8800 - loss: 0.8535 - val_accuracy: 0.9296 - val_loss: 0.6125\n",
            "171/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.9049 - loss: 0.7445 - val_accuracy: 0.9304 - val_loss: 0.5698\n",
            "172/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9068 - loss: 0.7312 - val_accuracy: 0.9318 - val_loss: 0.5827\n",
            "173/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.9048 - loss: 0.7220 - val_accuracy: 0.9322 - val_loss: 0.5470\n",
            "174/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.8961 - loss: 0.7172 - val_accuracy: 0.9326 - val_loss: 0.5353\n",
            "175/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.9029 - loss: 0.7591 - val_accuracy: 0.9372 - val_loss: 0.5112\n",
            "176/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.8925 - loss: 0.7582 - val_accuracy: 0.9334 - val_loss: 0.5465\n",
            "177/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9042 - loss: 0.7152 - val_accuracy: 0.9344 - val_loss: 0.5437\n",
            "178/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9053 - loss: 0.6826 - val_accuracy: 0.9372 - val_loss: 0.5200\n",
            "179/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.9133 - loss: 0.6856 - val_accuracy: 0.9378 - val_loss: 0.5113\n",
            "180/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.9198 - loss: 0.5971 - val_accuracy: 0.9344 - val_loss: 0.5261\n",
            "181/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.9203 - loss: 0.6327 - val_accuracy: 0.9438 - val_loss: 0.5107\n",
            "182/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.9104 - loss: 0.6772 - val_accuracy: 0.9432 - val_loss: 0.4701\n",
            "183/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.9143 - loss: 0.6397 - val_accuracy: 0.9404 - val_loss: 0.4563\n",
            "184/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.9252 - loss: 0.5757 - val_accuracy: 0.9362 - val_loss: 0.5368\n",
            "185/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.9103 - loss: 0.6414 - val_accuracy: 0.9362 - val_loss: 0.4859\n",
            "186/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9104 - loss: 0.6743 - val_accuracy: 0.9422 - val_loss: 0.4604\n",
            "187/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9163 - loss: 0.6164 - val_accuracy: 0.9376 - val_loss: 0.5154\n",
            "188/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9210 - loss: 0.6267 - val_accuracy: 0.9430 - val_loss: 0.4717\n",
            "189/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9030 - loss: 0.6794 - val_accuracy: 0.9454 - val_loss: 0.4321\n",
            "190/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - accuracy: 0.9193 - loss: 0.5979 - val_accuracy: 0.9390 - val_loss: 0.5117\n",
            "191/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.9182 - loss: 0.6891 - val_accuracy: 0.9400 - val_loss: 0.4870\n",
            "192/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.6630 - val_accuracy: 0.9432 - val_loss: 0.4100\n",
            "193/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9215 - loss: 0.6112 - val_accuracy: 0.9446 - val_loss: 0.4204\n",
            "194/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.9243 - loss: 0.5614 - val_accuracy: 0.9486 - val_loss: 0.4211\n",
            "195/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.9264 - loss: 0.5485 - val_accuracy: 0.9546 - val_loss: 0.3678\n",
            "196/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - accuracy: 0.9334 - loss: 0.4751 - val_accuracy: 0.9410 - val_loss: 0.4438\n",
            "197/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.9303 - loss: 0.5674 - val_accuracy: 0.9426 - val_loss: 0.4813\n",
            "198/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.9138 - loss: 0.6050 - val_accuracy: 0.9420 - val_loss: 0.4322\n",
            "199/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.9151 - loss: 0.6204 - val_accuracy: 0.9462 - val_loss: 0.4345\n",
            "200/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9279 - loss: 0.5541 - val_accuracy: 0.9370 - val_loss: 0.5110\n",
            "201/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9288 - loss: 0.5478 - val_accuracy: 0.9526 - val_loss: 0.3670\n",
            "202/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9204 - loss: 0.5972 - val_accuracy: 0.9536 - val_loss: 0.3695\n",
            "203/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.9290 - loss: 0.5449 - val_accuracy: 0.9544 - val_loss: 0.3353\n",
            "204/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9312 - loss: 0.5095 - val_accuracy: 0.9572 - val_loss: 0.3570\n",
            "205/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.9438 - loss: 0.4180 - val_accuracy: 0.9388 - val_loss: 0.4802\n",
            "206/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.9361 - loss: 0.4921 - val_accuracy: 0.9508 - val_loss: 0.3855\n",
            "207/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.9354 - loss: 0.4749 - val_accuracy: 0.9492 - val_loss: 0.4055\n",
            "208/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.9405 - loss: 0.4873 - val_accuracy: 0.9504 - val_loss: 0.4014\n",
            "209/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - accuracy: 0.9057 - loss: 0.7195 - val_accuracy: 0.9572 - val_loss: 0.3622\n",
            "210/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9293 - loss: 0.5043 - val_accuracy: 0.9530 - val_loss: 0.3593\n",
            "211/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9243 - loss: 0.5503 - val_accuracy: 0.9372 - val_loss: 0.4229\n",
            "212/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.9283 - loss: 0.5241 - val_accuracy: 0.9472 - val_loss: 0.3867\n",
            "213/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.9277 - loss: 0.4937 - val_accuracy: 0.9550 - val_loss: 0.3829\n",
            "214/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.9274 - loss: 0.5309 - val_accuracy: 0.9532 - val_loss: 0.3894\n",
            "215/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.9216 - loss: 0.6250 - val_accuracy: 0.9572 - val_loss: 0.3367\n",
            "216/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.9356 - loss: 0.4924 - val_accuracy: 0.9598 - val_loss: 0.3095\n",
            "217/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9319 - loss: 0.5479 - val_accuracy: 0.9626 - val_loss: 0.3010\n",
            "218/300\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.9384 - loss: 0.4781 - val_accuracy: 0.9592 - val_loss: 0.3322\n",
            "219/300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UI61hVlyQLS"
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP4vzcHwOmbP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "ae768bcb-5848-4b82-b32e-ab8b448f009e"
      },
      "source": [
        "environment = Environment(data, embeddings, alpha, gamma, fixed_length)\n",
        "\n",
        "tf.reset_default_graph() # For multiple consecutive executions\n",
        "\n",
        "sess = tf.Session()\n",
        "# '1: Initialize actor network f_θ^π and critic network Q(s, a|θ^µ) with random weights'\n",
        "actor = Actor(sess, state_space_size, action_space_size, batch_size, ra_length, history_length, embeddings.size(), tau, actor_lr)\n",
        "critic = Critic(sess, state_space_size, action_space_size, history_length, embeddings.size(), tau, critic_lr)\n",
        "\n",
        "train(sess, environment, actor, critic, embeddings, history_length, ra_length, buffer_size, batch_size, discount_factor, nb_episodes, filename_summary)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-2b61e1f64844>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For multiple consecutive executions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOnK_UxHzaPa"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsJ0klBBSSXn"
      },
      "source": [
        "dict_embeddings = {}\n",
        "for i, item in enumerate(embeddings.get_embedding_vector()):\n",
        "  str_item = str(item)\n",
        "  assert(str_item not in dict_embeddings)\n",
        "  dict_embeddings[str_item] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-LsmlHnyouJ"
      },
      "source": [
        "def state_to_items(state, actor, ra_length, embeddings, dict_embeddings, target=False):\n",
        "  return [dict_embeddings[str(action)]\n",
        "          for action in actor.get_recommendation_list(ra_length, np.array(state).reshape(1, -1), embeddings, target).reshape(ra_length, embeddings.size())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnCn8bf58uAP"
      },
      "source": [
        "def test_actor(actor, test_df, embeddings, dict_embeddings, ra_length, history_length, target=False, nb_rounds=1):\n",
        "  ratings = []\n",
        "  unknown = 0\n",
        "  random_seen = []\n",
        "  for _ in range(nb_rounds):\n",
        "    for i in range(len(test_df)):\n",
        "      history_sample = list(test_df[i].sample(history_length)['itemId'])\n",
        "      recommendation = state_to_items(embeddings.embed(history_sample), actor, ra_length, embeddings, dict_embeddings, target)\n",
        "      for item in recommendation:\n",
        "        l = list(test_df[i].loc[test_df[i]['itemId'] == item]['rating'])\n",
        "        assert(len(l) < 2)\n",
        "        if len(l) == 0:\n",
        "          unknown += 1\n",
        "        else:\n",
        "          ratings.append(l[0])\n",
        "      for item in history_sample:\n",
        "        random_seen.append(list(test_df[i].loc[test_df[i]['itemId'] == item]['rating'])[0])\n",
        "\n",
        "  return ratings, unknown, random_seen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0j3nHjUAP24"
      },
      "source": [
        "## Train set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBZr_opifZss"
      },
      "source": [
        "#### Target = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgqFYAsd8W-S"
      },
      "source": [
        "ratings, unknown, random_seen = test_actor(actor, dg.train, embeddings, dict_embeddings, ra_length, history_length, target=False, nb_rounds=10)\n",
        "print('%0.1f%% unknown' % (100 * unknown / (len(ratings) + unknown)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGetmkeD95rP"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(ratings)\n",
        "plt.title('Predictions ; Mean = %.4f' % (np.mean(ratings)))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(random_seen)\n",
        "plt.title('Random ; Mean = %.4f' % (np.mean(random_seen)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FPKh8gafdcn"
      },
      "source": [
        "#### Target = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvjYl_LTfGi9"
      },
      "source": [
        "ratings, unknown, random_seen = test_actor(actor, dg.train, embeddings, dict_embeddings, ra_length, history_length, target=True, nb_rounds=10)\n",
        "print('%0.1f%% unknown' % (100 * unknown / (len(ratings) + unknown)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IKYVQ6gfIXk"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(ratings)\n",
        "plt.title('Predictions ; Mean = %.4f' % (np.mean(ratings)))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(random_seen)\n",
        "plt.title('Random ; Mean = %.4f' % (np.mean(random_seen)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFimdOkLASAn"
      },
      "source": [
        "## Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x49sZSZIfgfN"
      },
      "source": [
        "#### Target = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXhBwo6t-p5R"
      },
      "source": [
        "ratings, unknown, random_seen = test_actor(actor, dg.test, embeddings, dict_embeddings, ra_length, history_length, target=False, nb_rounds=100)\n",
        "print('%0.1f%% unknown' % (100 * unknown / (len(ratings) + unknown)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFPjENrZAVsc"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(ratings)\n",
        "plt.title('Predictions ; Mean = %.4f' % (np.mean(ratings)))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(random_seen)\n",
        "plt.title('Random ; Mean = %.4f' % (np.mean(random_seen)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg4QPFo7fkzn"
      },
      "source": [
        "#### Target = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HCCXmcsAYkK"
      },
      "source": [
        "ratings, unknown, random_seen = test_actor(actor, dg.test, embeddings, dict_embeddings, ra_length, history_length, target=True, nb_rounds=100)\n",
        "print('%0.1f%% unknown' % (100 * unknown / (len(ratings) + unknown)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5T4zKcJfTa1"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(ratings)\n",
        "plt.title('Predictions ; Mean = %.4f' % (np.mean(ratings)))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(random_seen)\n",
        "plt.title('Random ; Mean = %.4f' % (np.mean(random_seen)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVKRoOm7k_fn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}